# ğŸš€ Real-time Financial Data Pipeline

A production-grade, scalable real-time data pipeline for ingesting, processing, and visualizing financial market data. This project demonstrates end-to-end data engineering skills including streaming data ingestion, ETL processes, data warehousing, and real-time analytics.

![Python](https://img.shields.io/badge/Python-3.9+-blue)
![Apache Kafka](https://img.shields.io/badge/Apache%20Kafka-Latest-black)
![Apache Spark](https://img.shields.io/badge/Apache%20Spark-3.5-orange)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-blue)
![Docker](https://img.shields.io/badge/Docker-Latest-blue)

## ğŸ“‹ Table of Contents

- [Architecture Overview](#architecture-overview)
- [Features](#features)
- [Tech Stack](#tech-stack)
- [Prerequisites](#prerequisites)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Data Flow](#data-flow)
- [Monitoring](#monitoring)
- [Testing](#testing)
- [Troubleshooting](#troubleshooting)
- [Future Enhancements](#future-enhancements)

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Sources   â”‚
â”‚  - Alpha Vantageâ”‚
â”‚  - Yahoo Financeâ”‚
â”‚  - CoinGecko    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka Broker   â”‚  â—„â”€â”€ Message Queue
â”‚  (Pub/Sub)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Spark Streaming â”‚  â—„â”€â”€ Stream Processing
â”‚  - Aggregations â”‚      (Windowing, Metrics)
â”‚  - Technical    â”‚
â”‚    Indicators   â”‚
â”‚  - Anomaly      â”‚
â”‚    Detection    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL    â”‚â”€â”€â”€â”€â–ºâ”‚    Redis     â”‚
â”‚ (Data Warehouse)â”‚     â”‚   (Cache)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Streamlit     â”‚  â—„â”€â”€ Visualization
â”‚   Dashboard     â”‚      (Real-time UI)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Features

### 1. **Multi-Source Data Ingestion**
- Real-time stock prices from Yahoo Finance & Alpha Vantage
- Cryptocurrency prices from CoinGecko & Binance
- Fault-tolerant ingestion with automatic failover
- Rate limiting and API quota management

### 2. **Stream Processing**
- Real-time window aggregations (1-min, 5-min)
- Technical indicators:
  - Simple Moving Average (SMA)
  - Exponential Moving Average (EMA)
  - Bollinger Bands
  - Volatility calculations
- Statistical anomaly detection (3-sigma rule)
- Late data handling with watermarks

### 3. **Data Storage**
- **Hot Storage**: Redis for latest prices and caching
- **Warm Storage**: PostgreSQL for recent data and fast queries
- **Schema Evolution**: Versioned database migrations
- Optimized indexes for query performance

### 4. **Data Quality**
- Schema validation
- Duplicate detection
- Missing data handling
- Data freshness monitoring
- Automated quality metrics

### 5. **Visualization**
- Real-time price updates
- Interactive candlestick charts
- Volume analysis
- Technical indicator plots
- Anomaly alerts

## ğŸ› ï¸ Tech Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Data Ingestion** | Apache Kafka | Message broker for streaming data |
| **Stream Processing** | Apache Spark Streaming | Real-time data transformation |
| **Data Storage** | PostgreSQL | Primary data warehouse |
| **Caching** | Redis | Fast in-memory cache |
| **Containerization** | Docker, Docker Compose | Service orchestration |
| **Visualization** | Streamlit, Plotly | Interactive dashboards |
| **Testing** | Pytest | Unit and integration tests |
| **Programming** | Python 3.9+ | Primary language |

## ğŸ“¦ Prerequisites

### Required Software
- Docker Desktop (latest version)
- Python 3.9 or higher
- Git

### API Keys (Free Tier)
1. **Alpha Vantage**: https://www.alphavantage.co/support/#api-key
2. **CoinGecko**: No API key required for free tier

## ğŸš€ Installation

### Step 1: Clone the Repository

```bash
git clone https://github.com/yourusername/financial-data-pipeline.git
cd financial-data-pipeline
```

### Step 2: Environment Setup

```bash
# Create and activate virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Step 3: Configure Environment Variables

```bash
# Copy the example .env file
cp .env.example .env

# Edit .env and add your API keys
# ALPHA_VANTAGE_API_KEY=your_key_here
```

### Step 4: Start Infrastructure

```bash
# Start all services (Kafka, Postgres, Redis, Spark)
docker-compose up -d

# Verify all services are running
docker-compose ps
```

Expected output:
```
NAME            COMMAND                  SERVICE      STATUS
kafka           "/etc/confluent/..."    kafka        running
postgres-db     "docker-entrypoint..."  postgres     running
redis-cache     "docker-entrypoint..."  redis        running
spark-master    "/opt/bitnami/..."      spark-master running
```

### Step 5: Initialize Database

```bash
# Database schema is automatically initialized via docker-compose
# Verify tables exist
docker exec -it postgres-db psql -U dataeng -d financial_data -c "\dt"
```

## ğŸ“Š Usage

### Running the Complete Pipeline

#### Terminal 1: Start Stock Producer
```bash
python src/ingestion/stock_producer.py
```

#### Terminal 2: Start Crypto Producer
```bash
python src/ingestion/crypto_producer.py
```

#### Terminal 3: Start Spark Streaming
```bash
python src/processing/spark_streaming.py
```

#### Terminal 4: Launch Dashboard
```bash
streamlit run src/visualization/dashboard.py
```

Access the dashboard at: http://localhost:8501

### Running Individual Components

#### Test Data Ingestion (Single Batch)
```bash
python -c "from src.ingestion.stock_producer import StockDataProducer; StockDataProducer().run_once()"
```

#### Check Kafka Topics
```bash
# Access Kafka UI
open http://localhost:8090
```

#### Query Database
```bash
docker exec -it postgres-db psql -U dataeng -d financial_data

# Example queries
SELECT * FROM stock_prices_raw ORDER BY timestamp DESC LIMIT 10;
SELECT * FROM price_anomalies;
```

#### Check Redis Cache
```bash
docker exec -it redis-cache redis-cli

# Example commands
> KEYS *
> GET price:AAPL
```

## ğŸ“ Project Structure

```
financial-data-pipeline/
â”œâ”€â”€ docker-compose.yml           # Infrastructure orchestration
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ .env                         # Environment variables
â”œâ”€â”€ README.md                    # This file
â”‚
â”œâ”€â”€ config/                      # Configuration files
â”‚   â”œâ”€â”€ kafka_config.yaml
â”‚   â””â”€â”€ spark_config.yaml
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/              # Data ingestion layer
â”‚   â”‚   â”œâ”€â”€ stock_producer.py   # Stock data producer
â”‚   â”‚   â””â”€â”€ crypto_producer.py  # Crypto data producer
â”‚   â”‚
â”‚   â”œâ”€â”€ processing/             # Stream processing
â”‚   â”‚   â”œâ”€â”€ spark_streaming.py  # Spark streaming job
â”‚   â”‚   â””â”€â”€ data_transformer.py # Data transformations
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/                # Data storage layer
â”‚   â”‚   â”œâ”€â”€ db_manager.py       # Database operations
â”‚   â”‚   â””â”€â”€ schema.sql          # Database schema
â”‚   â”‚
â”‚   â””â”€â”€ visualization/          # Visualization layer
â”‚       â””â”€â”€ dashboard.py        # Streamlit dashboard
â”‚
â”œâ”€â”€ tests/                      # Test suite
â”‚   â”œâ”€â”€ test_ingestion.py
â”‚   â”œâ”€â”€ test_processing.py
â”‚   â””â”€â”€ test_storage.py
â”‚
â””â”€â”€ docs/                       # Additional documentation
    â”œâ”€â”€ ARCHITECTURE.md
    â””â”€â”€ API.md
```

## ğŸ”„ Data Flow

### 1. **Ingestion Phase**
```
APIs â†’ Producers â†’ Kafka Topics
```
- Producers fetch data every 60 seconds (configurable)
- Data validated before publishing to Kafka
- Failed requests automatically retry with backoff

### 2. **Processing Phase**
```
Kafka â†’ Spark Streaming â†’ Transformations â†’ Database
```
- Spark reads micro-batches from Kafka
- Applies windowed aggregations
- Calculates technical indicators
- Detects anomalies
- Writes to PostgreSQL

### 3. **Storage Phase**
```
Spark â†’ PostgreSQL (Warehouse) + Redis (Cache)
```
- Raw data stored in fact tables
- Aggregated metrics in dimension tables
- Latest prices cached in Redis

### 4. **Visualization Phase**
```
Database â†’ Streamlit â†’ User Interface
```
- Dashboard queries database
- Auto-refresh every 10 seconds
- Interactive charts and metrics

## ğŸ“ˆ Monitoring

### Kafka UI
Access at http://localhost:8090
- View topics and messages
- Monitor consumer lag
- Check partition distribution

### Spark UI
Access at http://localhost:8080
- Monitor streaming jobs
- View job statistics
- Check resource usage

### Database Metrics
```python
from src.storage.db_manager import DatabaseManager

db = DatabaseManager()
metrics = db.get_data_quality_metrics()
print(metrics)
```

### Logs
```bash
# View logs for specific service
docker-compose logs -f kafka
docker-compose logs -f spark-master
docker-compose logs -f postgres
```

## ğŸ§ª Testing

### Run All Tests
```bash
pytest tests/ -v
```

### Run Specific Test Suite
```bash
# Test ingestion
pytest tests/test_ingestion.py -v

# Test with coverage
pytest tests/ --cov=src --cov-report=html
```

### Manual Testing
```bash
# Test database connection
python -c "from src.storage.db_manager import DatabaseManager; DatabaseManager().get_data_quality_metrics()"

# Test Redis cache
python -c "from src.storage.db_manager import CacheManager; CacheManager().set_latest_price('TEST', 100.0)"
```

## ğŸ› Troubleshooting

### Common Issues

#### 1. Kafka Connection Error
```bash
# Check if Kafka is running
docker-compose ps kafka

# Restart Kafka
docker-compose restart kafka
```

#### 2. Database Connection Error
```bash
# Verify database is running
docker exec -it postgres-db pg_isready

# Check connection
docker exec -it postgres-db psql -U dataeng -d financial_data -c "SELECT 1;"
```

#### 3. API Rate Limits
- Alpha Vantage: 5 calls/minute (free tier)
- Solution: Increase `INGESTION_INTERVAL_SECONDS` in `.env`

#### 4. Spark Out of Memory
```yaml
# Increase memory in docker-compose.yml
spark-worker:
  environment:
    - SPARK_WORKER_MEMORY=4G
```

#### 5. Port Already in Use
```bash
# Find process using port
lsof -i :9092  # Kafka
lsof -i :5432  # PostgreSQL

# Kill process or change port in docker-compose.yml
```

## ğŸš€ Future Enhancements

### Short-term
- [ ] Add Apache Airflow for orchestration
- [ ] Implement data lineage tracking
- [ ] Add alerting system (email/Slack)
- [ ] Create API endpoint for data access

### Medium-term
- [ ] Migrate to cloud (AWS/GCP)
- [ ] Implement data partitioning strategy
- [ ] Add machine learning predictions
- [ ] Create mobile dashboard

### Long-term
- [ ] Implement multi-region deployment
- [ ] Add blockchain data sources
- [ ] Build recommendation engine
- [ ] Create data marketplace

## ğŸ“ Key Learnings for Interview

This project demonstrates:

1. **System Design**: Designed a scalable, fault-tolerant architecture
2. **Stream Processing**: Handled real-time data with low latency
3. **Data Quality**: Implemented validation and monitoring
4. **Performance**: Optimized for throughput and low latency
5. **DevOps**: Containerized services for easy deployment
6. **Testing**: Unit tests with good coverage
7. **Documentation**: Clear, comprehensive docs

## ğŸ¤ Contributing

Feel free to fork, improve, and create pull requests!

## ğŸ“„ License

MIT License

---

**Created by**: [Your Name]  
**Email**: your.email@example.com  
**LinkedIn**: [Your LinkedIn]  
**GitHub**: [Your GitHub]

**Built for**: Data Engineer Position at LSEG  
**Date**: December 2024